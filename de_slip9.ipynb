{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gumKz3Kw8h4s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a53fcf0-f946-48f5-8262-8a8bfce7de90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working directory: /tmp/spark_rdd_demo_fixed\n",
            "\n",
            "=== 1) sc.parallelize ===\n",
            "Partitions: 2\n",
            "Data: [1, 2, 3, 4, 5]\n",
            "\n",
            "=== 2) sc.textFile ===\n",
            "Partitions: 3\n",
            "Line count: 9\n",
            "Sample lines: ['INFO log1: hello world', 'WARN log1: something to note', 'ERROR log1: something went wrong']\n",
            "\n",
            "=== 3) sc.wholeTextFiles ===\n",
            "Partitions: 2\n",
            "Files and sizes (bytes): [('db.conf', 26), ('app.conf', 20)]\n",
            "\n",
            "=== 4) sc.range ===\n",
            "Partitions: 4\n",
            "First 10 values: [0, 5, 10, 15, 20, 25, 30, 35, 40, 45]\n",
            "\n",
            "=== 5) sc.binaryFiles ===\n",
            "Partitions: 1\n",
            "Binary file sizes (bytes): [('data.bin', 2048), ('icon.png', 1024)]\n",
            "\n",
            "Spark session stopped and temporary directory cleaned up.\n"
          ]
        }
      ],
      "source": [
        "#1] Create a Spark RDD using 5 different Functions\n",
        "\n",
        "#Solution:\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from os.path import basename\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# --- Setup Spark Session and Context ---\n",
        "spark = SparkSession.builder.appName(\"RDD_Creation_Fixed\").master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# --- Create a working directory for our files ---\n",
        "WORK_DIR = \"/tmp/spark_rdd_demo_fixed\"\n",
        "if os.path.exists(WORK_DIR):\n",
        "    shutil.rmtree(WORK_DIR)\n",
        "os.makedirs(WORK_DIR)\n",
        "os.chdir(WORK_DIR)\n",
        "\n",
        "print(f\"Working directory: {os.getcwd()}\\n\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 1) sc.parallelize\n",
        "# ==============================================================================\n",
        "print(\"=== 1) sc.parallelize ===\")\n",
        "rdd1 = sc.parallelize([1, 2, 3, 4, 5], 2)\n",
        "print(f\"Partitions: {rdd1.getNumPartitions()}\")\n",
        "print(f\"Data: {rdd1.collect()}\\n\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2) sc.textFile\n",
        "# ==============================================================================\n",
        "print(\"=== 2) sc.textFile ===\")\n",
        "log_file_path = os.path.join(WORK_DIR, \"logs.txt\")\n",
        "with open(log_file_path, \"w\") as f:\n",
        "    for i in range(1, 4):\n",
        "        f.write(f\"INFO log{i}: hello world\\n\")\n",
        "        f.write(f\"WARN log{i}: something to note\\n\")\n",
        "        f.write(f\"ERROR log{i}: something went wrong\\n\")\n",
        "\n",
        "rdd2 = sc.textFile(log_file_path, 3)\n",
        "print(f\"Partitions: {rdd2.getNumPartitions()}\")\n",
        "print(f\"Line count: {rdd2.count()}\")\n",
        "print(f\"Sample lines: {rdd2.take(3)}\\n\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 3) sc.wholeTextFiles\n",
        "# ==============================================================================\n",
        "print(\"=== 3) sc.wholeTextFiles ===\")\n",
        "conf_dir = os.path.join(WORK_DIR, \"configs\")\n",
        "os.makedirs(conf_dir)\n",
        "with open(os.path.join(conf_dir, \"db.conf\"), \"w\") as f:\n",
        "    f.write(\"user=root\\npassword=secret\\n\")\n",
        "with open(os.path.join(conf_dir, \"app.conf\"), \"w\") as f:\n",
        "    f.write(\"threads=4\\nport=8080\\n\")\n",
        "\n",
        "rdd3 = sc.wholeTextFiles(conf_dir, 2)\n",
        "print(f\"Partitions: {rdd3.getNumPartitions()}\")\n",
        "files_and_sizes = rdd3.map(lambda kv: (basename(kv[0]), len(kv[1]))).collect()\n",
        "print(f\"Files and sizes (bytes): {files_and_sizes}\\n\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 4) sc.range\n",
        "# ==============================================================================\n",
        "print(\"=== 4) sc.range ===\")\n",
        "# Creates an RDD from 0 to 100 with a step of 5\n",
        "rdd4 = sc.range(0, 100, 5, 4)\n",
        "print(f\"Partitions: {rdd4.getNumPartitions()}\")\n",
        "print(f\"First 10 values: {rdd4.take(10)}\\n\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 5) sc.binaryFiles (FIXED)\n",
        "# ==============================================================================\n",
        "print(\"=== 5) sc.binaryFiles ===\")\n",
        "binary_dir = os.path.join(WORK_DIR, \"binaries\")\n",
        "os.makedirs(binary_dir)\n",
        "# Create a dummy binary file (e.g., a small image or just some bytes)\n",
        "with open(os.path.join(binary_dir, \"icon.png\"), \"wb\") as f:\n",
        "    f.write(os.urandom(1024)) # 1KB of random data\n",
        "with open(os.path.join(binary_dir, \"data.bin\"), \"wb\") as f:\n",
        "    f.write(os.urandom(2048)) # 2KB of random data\n",
        "\n",
        "rdd5 = sc.binaryFiles(binary_dir, 1)\n",
        "print(f\"Partitions: {rdd5.getNumPartitions()}\")\n",
        "\n",
        "# --- THE FIX IS APPLIED HERE ---\n",
        "# Instead of kv[1].read(), we use len(kv[1]) directly on the bytes object.\n",
        "sizes = rdd5.map(lambda kv: (basename(kv[0]), len(kv[1]))).collect()\n",
        "print(\"Binary file sizes (bytes):\", sizes)\n",
        "\n",
        "# ==============================================================================\n",
        "# --- Clean up ---\n",
        "# ==============================================================================\n",
        "sc.stop()\n",
        "shutil.rmtree(WORK_DIR)\n",
        "print(\"\\nSpark session stopped and temporary directory cleaned up.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67b9b26b"
      },
      "source": [
        "The error you are encountering, `PicklingError: Could not serialize object: IndexError: tuple index out of range`, often happens in Spark when it tries to send a function or object to its workers for execution, but the object cannot be properly serialized (or \"pickled\").\n",
        "\n",
        "In interactive environments like notebooks, this can be related to how the Spark context or session is created and how the functions are defined and executed within the notebook's scope. The `IndexError: tuple index out of range` within the `cloudpickle` library suggests an issue during the inspection of the function's code or globals for serialization.\n",
        "\n",
        "While the exact root cause can be complex and environment-dependent, ensuring the Spark session and context are correctly initialized and managed is crucial.\n",
        "\n",
        "Here's the code again, structured to minimize potential serialization issues in a notebook environment:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2] Write example for following Spark RDD Actions:\n",
        "#a. aggregate  b. treeAggregate c. fold\n",
        "#d. reduce  e. collect\n",
        "\n",
        "#Solution:\n",
        "\n",
        "# Spark RDD Actions Demo:\n",
        "# a) aggregate      -> aggregate with different input/output types (e.g., (sum, count) to compute avg)\n",
        "# b) treeAggregate  -> hierarchical/Tree-style aggregate (same API as aggregate + depth)\n",
        "# c) fold           -> like aggregate but zero value is same type as elements and uses one binary op\n",
        "# d) reduce         -> combine elements using an associative+commutative function\n",
        "# e) collect        -> bring all elements to driver (use only for small RDDs)\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add, mul\n",
        "\n",
        "spark = SparkSession.builder.appName(\"RDD Actions Demo\").master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "sc.setLogLevel(\"ERROR\")\n",
        "\n",
        "# Sample RDD\n",
        "rdd = sc.parallelize(list(range(1, 11)), numSlices=4)  # 1..10\n",
        "print(f\"RDD partitions: {rdd.getNumPartitions()}\")\n",
        "print(f\"RDD data (small preview via take): {rdd.take(5)}\")\n",
        "\n",
        "# a) aggregate: compute (sum, count) then average\n",
        "# zero must be a neutral element for both seqOp and combOp\n",
        "zero = (0, 0)\n",
        "\n",
        "def seq_op(acc, x):\n",
        "    s, c = acc\n",
        "    return (s + x, c + 1)\n",
        "\n",
        "def comb_op(a, b):\n",
        "    s1, c1 = a\n",
        "    s2, c2 = b\n",
        "    return (s1 + s2, c1 + c2)\n",
        "\n",
        "sum_count = rdd.aggregate(zero, seq_op, comb_op)\n",
        "avg_via_aggregate = sum_count[0] / sum_count[1]\n",
        "print(\"\\n[a] aggregate -> (sum, count):\", sum_count, \"average:\", avg_via_aggregate)\n",
        "\n",
        "# b) treeAggregate: same logic, aggregated in a tree pattern (set depth as needed)\n",
        "sum_count_tree = rdd.treeAggregate(zero, seq_op, comb_op, depth=3)\n",
        "avg_via_treeAggregate = sum_count_tree[0] / sum_count_tree[1]\n",
        "print(\"[b] treeAggregate -> (sum, count):\", sum_count_tree, \"average:\", avg_via_treeAggregate)\n",
        "\n",
        "# c) fold: single associative op, zero must be identity for the op and same type as elements\n",
        "fold_sum = rdd.fold(0, add)           # identity for + is 0\n",
        "fold_prod = rdd.fold(1, mul)          # identity for * is 1\n",
        "print(\"[c] fold -> sum:\", fold_sum, \"product:\", fold_prod)\n",
        "\n",
        "# d) reduce: combine elements with a binary op\n",
        "reduce_sum = rdd.reduce(add)\n",
        "reduce_max = rdd.reduce(max)\n",
        "print(\"[d] reduce -> sum:\", reduce_sum, \"max:\", reduce_max)\n",
        "\n",
        "# e) collect: bring all elements to the driver (only for small RDDs)\n",
        "all_data = rdd.collect()\n",
        "print(\"[e] collect ->\", all_data)\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "6PD_HTLKRxrP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a652f20-2c6e-46a6-ee42-e2178db3a249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD partitions: 4\n",
            "RDD data (small preview via take): [1, 2, 3, 4, 5]\n",
            "\n",
            "[a] aggregate -> (sum, count): (55, 10) average: 5.5\n",
            "[b] treeAggregate -> (sum, count): (55, 10) average: 5.5\n",
            "[c] fold -> sum: 55 product: 3628800\n",
            "[d] reduce -> sum: 55 max: 10\n",
            "[e] collect -> [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GBZOhMhZjfxP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}