{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mCsEVMbfyV1",
        "outputId": "3e6874eb-2431-4bce-c772-07dc2aad00be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Created file1.txt and file2.txt successfully.\n",
            "\n",
            "--- Contents of the single RDD ---\n",
            "Hello Spark from file one\n",
            "This is the first line\n",
            "End of file one\n",
            "Greetings from file two\n",
            "This is the second line\n"
          ]
        }
      ],
      "source": [
        "#1] a) Create 2 text files. Read the contents in a single RDD.\n",
        "\n",
        "#Solution:\n",
        "\n",
        "# 1. Install and set up Spark\n",
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"ReadMultipleFiles\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# 2. Create the two text files\n",
        "with open(\"file1.txt\", \"w\") as f:\n",
        "  f.write(\"Hello Spark from file one\\n\")\n",
        "  f.write(\"This is the first line\\n\")\n",
        "  f.write(\"End of file one\\n\")\n",
        "\n",
        "with open(\"file2.txt\", \"w\") as f:\n",
        "  f.write(\"Greetings from file two\\n\")\n",
        "  f.write(\"This is the second line\\n\")\n",
        "\n",
        "print(\"Created file1.txt and file2.txt successfully.\")\n",
        "\n",
        "# 3. Read both files into a single RDD using a wildcard\n",
        "rdd = sc.textFile(\"file*.txt\")\n",
        "\n",
        "# 4. Collect and print the RDD's content to verify\n",
        "print(\"\\n--- Contents of the single RDD ---\")\n",
        "for line in rdd.collect():\n",
        "  print(line)\n",
        "\n",
        "# 5. Stop the SparkSession\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1] b) Create 2 CSV files. Read the contents in a single RDD.\n",
        "\n",
        "#Solution:\n",
        "\n",
        "# 1. Install and set up Spark\n",
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"ReadMultipleCSVs\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# 2. Create the two CSV files\n",
        "with open(\"products.csv\", \"w\") as f:\n",
        "  f.write(\"product_id,product_name,price\\n\")\n",
        "  f.write(\"101,Laptop,1200\\n\")\n",
        "  f.write(\"102,Mouse,25\\n\")\n",
        "\n",
        "with open(\"more_products.csv\", \"w\") as f:\n",
        "  f.write(\"product_id,product_name,price\\n\")\n",
        "  f.write(\"103,Keyboard,75\\n\")\n",
        "  f.write(\"104,Webcam,50\\n\")\n",
        "\n",
        "print(\"Created products.csv and more_products.csv successfully.\")\n",
        "\n",
        "# 3. Read all files ending with .csv into a single RDD\n",
        "csv_rdd = sc.textFile(\"*.csv\")\n",
        "\n",
        "# 4. Collect and print the RDD's content to verify\n",
        "print(\"\\n--- Contents of the single RDD from both CSVs ---\")\n",
        "for line in csv_rdd.collect():\n",
        "  print(line)\n",
        "\n",
        "# 5. Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhsEP7vXgA5o",
        "outputId": "49d2d05f-b15d-483b-ffc1-57d39644c37d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Created products.csv and more_products.csv successfully.\n",
            "\n",
            "--- Contents of the single RDD from both CSVs ---\n",
            "product_id,product_name,price\n",
            "103,Keyboard,75\n",
            "104,Webcam,50\n",
            "product_id,product_name,price\n",
            "101,Laptop,1200\n",
            "102,Mouse,25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2] Create two dataframes one for employee and other for dept. Perform\n",
        "#a) Left outer join\n",
        "#b) Full outer join\n",
        "#c) Inner join\n",
        "\n",
        "#Solution:\n",
        "\n",
        "# Install PySpark\n",
        "!pip install pyspark\n",
        "\n",
        "# Import and create a SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"JoinExamples\").getOrCreate()\n",
        "\n",
        "# Employee Data\n",
        "emp_data = [\n",
        "    (1, \"Alice\", 10),\n",
        "    (2, \"Bob\", 20),\n",
        "    (3, \"Charlie\", 10),\n",
        "    (4, \"David\", None),      # Employee with no department\n",
        "    (5, \"Eve\", 40)           # Employee in a non-existent department\n",
        "]\n",
        "emp_columns = [\"emp_id\", \"emp_name\", \"dept_id\"]\n",
        "empDF = spark.createDataFrame(data=emp_data, schema=emp_columns)\n",
        "\n",
        "# Department Data\n",
        "dept_data = [\n",
        "    (10, \"Engineering\"),\n",
        "    (20, \"Marketing\"),\n",
        "    (30, \"Finance\")        # Department with no employees\n",
        "]\n",
        "dept_columns = [\"dept_id\", \"dept_name\"]\n",
        "deptDF = spark.createDataFrame(data=dept_data, schema=dept_columns)\n",
        "\n",
        "print(\"Employee DataFrame:\")\n",
        "empDF.show()\n",
        "\n",
        "print(\"Department DataFrame:\")\n",
        "deptDF.show()\n",
        "\n",
        "#a) Left outer join\n",
        "print(\"--- a) Left Outer Join ---\")\n",
        "# Keep all employees, and add department info where it exists\n",
        "left_join_df = empDF.join(deptDF, on=\"dept_id\", how=\"left\")\n",
        "left_join_df.show()\n",
        "\n",
        "#b) Full outer join\n",
        "print(\"--- b) Full Outer Join ---\")\n",
        "# Keep all records from both DataFrames\n",
        "full_outer_join_df = empDF.join(deptDF, on=\"dept_id\", how=\"full\")\n",
        "full_outer_join_df.show()\n",
        "\n",
        "#c) Inner join\n",
        "print(\"--- c) Inner Join ---\")\n",
        "# Keep only records that have a match in both DataFrames\n",
        "inner_join_df = empDF.join(deptDF, on=\"dept_id\", how=\"inner\")\n",
        "inner_join_df.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU1M2QBLgqMN",
        "outputId": "7b6295df-387e-48b4-8f93-30f1f4f4415a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Employee DataFrame:\n",
            "+------+--------+-------+\n",
            "|emp_id|emp_name|dept_id|\n",
            "+------+--------+-------+\n",
            "|     1|   Alice|     10|\n",
            "|     2|     Bob|     20|\n",
            "|     3| Charlie|     10|\n",
            "|     4|   David|   NULL|\n",
            "|     5|     Eve|     40|\n",
            "+------+--------+-------+\n",
            "\n",
            "Department DataFrame:\n",
            "+-------+-----------+\n",
            "|dept_id|  dept_name|\n",
            "+-------+-----------+\n",
            "|     10|Engineering|\n",
            "|     20|  Marketing|\n",
            "|     30|    Finance|\n",
            "+-------+-----------+\n",
            "\n",
            "--- a) Left Outer Join ---\n",
            "+-------+------+--------+-----------+\n",
            "|dept_id|emp_id|emp_name|  dept_name|\n",
            "+-------+------+--------+-----------+\n",
            "|     10|     1|   Alice|Engineering|\n",
            "|     20|     2|     Bob|  Marketing|\n",
            "|   NULL|     4|   David|       NULL|\n",
            "|     10|     3| Charlie|Engineering|\n",
            "|     40|     5|     Eve|       NULL|\n",
            "+-------+------+--------+-----------+\n",
            "\n",
            "--- b) Full Outer Join ---\n",
            "+-------+------+--------+-----------+\n",
            "|dept_id|emp_id|emp_name|  dept_name|\n",
            "+-------+------+--------+-----------+\n",
            "|   NULL|     4|   David|       NULL|\n",
            "|     10|     1|   Alice|Engineering|\n",
            "|     10|     3| Charlie|Engineering|\n",
            "|     20|     2|     Bob|  Marketing|\n",
            "|     30|  NULL|    NULL|    Finance|\n",
            "|     40|     5|     Eve|       NULL|\n",
            "+-------+------+--------+-----------+\n",
            "\n",
            "--- c) Inner Join ---\n",
            "+-------+------+--------+-----------+\n",
            "|dept_id|emp_id|emp_name|  dept_name|\n",
            "+-------+------+--------+-----------+\n",
            "|     10|     1|   Alice|Engineering|\n",
            "|     10|     3| Charlie|Engineering|\n",
            "|     20|     2|     Bob|  Marketing|\n",
            "+-------+------+--------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4n-VNbTNixzZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}