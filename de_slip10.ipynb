{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUkSlFukp2uF",
        "outputId": "94560e53-2733-433f-ed69-8ce7d3157b4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partitions: 4\n",
            "[a] count -> 200\n",
            "[b] countApproxDistinct -> 145  (true distinct is 150)\n",
            "[c] first -> 1\n",
            "[d] top(5) -> [150, 149, 148, 147, 146]\n",
            "[e] min -> 1\n"
          ]
        }
      ],
      "source": [
        "#1] Write example for following Spark RDD Actions:\n",
        "#a. count   b. countApproxDistinct\n",
        "#c. first   d. top   e. Min\n",
        "\n",
        "#Solution:\n",
        "\n",
        "# Spark RDD Actions Demo (single script)\n",
        "# Actions covered:\n",
        "# a) count\n",
        "# b) countApproxDistinct\n",
        "# c) first\n",
        "# d) top\n",
        "# e) min\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def main():\n",
        "    spark = SparkSession.builder.appName(\"RDD Actions Demo #1\").master(\"local[*]\").getOrCreate()\n",
        "    sc = spark.sparkContext\n",
        "    sc.setLogLevel(\"ERROR\")\n",
        "\n",
        "    # Example RDD with duplicates: 1..100 plus 51..150\n",
        "    nums = list(range(1, 101)) + list(range(51, 151))\n",
        "    rdd = sc.parallelize(nums, numSlices=4)\n",
        "\n",
        "    print(f\"Partitions: {rdd.getNumPartitions()}\")\n",
        "\n",
        "    # a) count\n",
        "    total_count = rdd.count()\n",
        "    print(f\"[a] count -> {total_count}\")  # expected 200\n",
        "\n",
        "    # b) countApproxDistinct (approximate distinct count)\n",
        "    approx_distinct = rdd.countApproxDistinct()  # default relativeSD=0.05\n",
        "    print(f\"[b] countApproxDistinct -> {approx_distinct}  (true distinct is 150)\")\n",
        "\n",
        "    # c) first\n",
        "    first_elem = rdd.first()\n",
        "    print(f\"[c] first -> {first_elem}\")\n",
        "\n",
        "    # d) top (descending)\n",
        "    top5 = rdd.top(5)\n",
        "    print(f\"[d] top(5) -> {top5}\")\n",
        "\n",
        "    # e) min\n",
        "    min_val = rdd.min()\n",
        "    print(f\"[e] min -> {min_val}\")\n",
        "\n",
        "    spark.stop()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Write Spark Pair RDD Functions.\n",
        "\n",
        "#Solution:\n"
      ],
      "metadata": {
        "id": "4EIE8Kt9q9mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark Pair RDD Functions Demo (single script)\n",
        "# Covered:\n",
        "# - Transformations: keys, values, mapValues, flatMapValues, reduceByKey, foldByKey,\n",
        "#   aggregateByKey, combineByKey, groupByKey, sortByKey, subtractByKey, join,\n",
        "#   leftOuterJoin, rightOuterJoin, fullOuterJoin, cogroup, partitionBy,\n",
        "#   repartitionAndSortWithinPartitions\n",
        "# - Actions: countByKey, lookup\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "def main():\n",
        "    spark = SparkSession.builder.appName(\"Pair RDD Functions Demo\").master(\"local[*]\").getOrCreate()\n",
        "    sc = spark.sparkContext\n",
        "    sc.setLogLevel(\"ERROR\")\n",
        "\n",
        "    # Sample Pair RDDs\n",
        "    data = [\n",
        "        (\"a\", 1), (\"b\", 2), (\"a\", 3), (\"c\", 5),\n",
        "        (\"b\", 1), (\"a\", 2), (\"d\", 4)\n",
        "    ]\n",
        "    info = [\n",
        "        (\"a\", \"alpha\"), (\"b\", \"beta\"), (\"d\", \"delta\"), (\"e\", \"epsilon\")\n",
        "    ]\n",
        "\n",
        "    kv = sc.parallelize(data, numSlices=3)\n",
        "    kv2 = sc.parallelize(info, numSlices=2)\n",
        "\n",
        "    print(f\"kv partitions: {kv.getNumPartitions()}, kv2 partitions: {kv2.getNumPartitions()}\")\n",
        "    print(\"kv ->\", kv.collect())\n",
        "    print(\"kv2 ->\", kv2.collect())\n",
        "\n",
        "    # keys, values\n",
        "    print(\"\\n-- keys(), values() --\")\n",
        "    print(\"keys():\", kv.keys().collect())\n",
        "    print(\"values():\", kv.values().collect())\n",
        "\n",
        "    # mapValues, flatMapValues\n",
        "    print(\"\\n-- mapValues, flatMapValues --\")\n",
        "    print(\"mapValues(*10):\", kv.mapValues(lambda v: v * 10).collect())\n",
        "    print(\"flatMapValues(range(1, v+1)) sample:\", kv.flatMapValues(lambda v: range(1, v + 1)).take(12))\n",
        "\n",
        "    # reduceByKey, foldByKey\n",
        "    print(\"\\n-- reduceByKey, foldByKey --\")\n",
        "    bykey_sum = kv.reduceByKey(add)\n",
        "    print(\"reduceByKey(sum):\", sorted(bykey_sum.collect()))\n",
        "    fold_sum = kv.foldByKey(0, add)\n",
        "    print(\"foldByKey(sum, zero=0):\", sorted(fold_sum.collect()))\n",
        "\n",
        "    # aggregateByKey: (sum, count) -> avg\n",
        "    print(\"\\n-- aggregateByKey (avg per key) --\")\n",
        "    agg = kv.aggregateByKey((0, 0),\n",
        "                            lambda acc, v: (acc[0] + v, acc[1] + 1),\n",
        "                            lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
        "    avg_by_key = agg.mapValues(lambda sc: sc[0] / sc[1])\n",
        "    print(\"aggregateByKey -> (sum,count):\", sorted(agg.collect()))\n",
        "    print(\"avg_by_key:\", sorted(avg_by_key.collect()))\n",
        "\n",
        "    # combineByKey: another way to compute avg\n",
        "    print(\"\\n-- combineByKey (avg per key) --\")\n",
        "    comb = kv.combineByKey(lambda v: (v, 1),\n",
        "                           lambda acc, v: (acc[0] + v, acc[1] + 1),\n",
        "                           lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
        "    avg_by_key2 = comb.mapValues(lambda sc: sc[0] / sc[1])\n",
        "    print(\"combineByKey -> (sum,count):\", sorted(comb.collect()))\n",
        "    print(\"avg_by_key2:\", sorted(avg_by_key2.collect()))\n",
        "\n",
        "    # groupByKey (use with care; can be heavy)\n",
        "    print(\"\\n-- groupByKey --\")\n",
        "    grouped = kv.groupByKey().mapValues(lambda it: sorted(list(it)))\n",
        "    print(\"groupByKey ->\", sorted(grouped.collect()))\n",
        "\n",
        "    # sortByKey\n",
        "    print(\"\\n-- sortByKey --\")\n",
        "    print(\"sortByKey(asc):\", kv.sortByKey(ascending=True).collect())\n",
        "    print(\"sortByKey(desc):\", kv.sortByKey(ascending=False).collect())\n",
        "\n",
        "    # subtractByKey\n",
        "    print(\"\\n-- subtractByKey --\")\n",
        "    print(\"kv.subtractByKey(kv2):\", kv.subtractByKey(kv2).collect())\n",
        "\n",
        "    # Joins\n",
        "    print(\"\\n-- joins --\")\n",
        "    print(\"join:\", sorted(kv.join(kv2).collect(), key=lambda x: (x[0], x[1])))\n",
        "    print(\"leftOuterJoin:\", sorted(kv.leftOuterJoin(kv2).collect(), key=lambda x: x[0]))\n",
        "    print(\"rightOuterJoin:\", sorted(kv.rightOuterJoin(kv2).collect(), key=lambda x: x[0]))\n",
        "    print(\"fullOuterJoin:\", sorted(kv.fullOuterJoin(kv2).collect(), key=lambda x: x[0]))\n",
        "\n",
        "    # cogroup (values from multiple RDDs grouped per key)\n",
        "    print(\"\\n-- cogroup --\")\n",
        "    co = kv.cogroup(kv2).mapValues(lambda t: (sorted(list(t[0])), sorted(list(t[1]))))\n",
        "    print(\"cogroup:\", sorted(co.collect()))\n",
        "\n",
        "    # Actions: countByKey, lookup\n",
        "    print(\"\\n-- actions: countByKey, lookup --\")\n",
        "    print(\"countByKey:\", dict(kv.countByKey()))\n",
        "    print(\"lookup('a'):\", kv.lookup(\"a\"))\n",
        "\n",
        "    # sampleByKey (probabilistic)\n",
        "    print(\"\\n-- sampleByKey --\")\n",
        "    fractions = {\"a\": 1.0, \"b\": 0.5, \"c\": 1.0, \"d\": 0.0}\n",
        "    sampled = kv.sampleByKey(withReplacement=False, fractions=fractions, seed=42)\n",
        "    print(\"sampleByKey:\", sorted(sampled.collect()))\n",
        "\n",
        "    # partitionBy and repartitionAndSortWithinPartitions\n",
        "    print(\"\\n-- partitionBy, repartitionAndSortWithinPartitions --\")\n",
        "    kv_part = kv.partitionBy(2)\n",
        "    print(\"partitionBy(2) -> partitions:\", kv_part.getNumPartitions())\n",
        "    rs = kv.repartitionAndSortWithinPartitions(2)\n",
        "    parts = rs.mapPartitionsWithIndex(lambda idx, it: [(idx, list(it))]).collect()\n",
        "    print(\"repartitionAndSortWithinPartitions(2):\", sorted(parts, key=lambda x: x[0]))\n",
        "\n",
        "    spark.stop()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKmNsnyQrQcY",
        "outputId": "ac5893de-1737-46c3-8024-3c4533d52f9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kv partitions: 3, kv2 partitions: 2\n",
            "kv -> [('a', 1), ('b', 2), ('a', 3), ('c', 5), ('b', 1), ('a', 2), ('d', 4)]\n",
            "kv2 -> [('a', 'alpha'), ('b', 'beta'), ('d', 'delta'), ('e', 'epsilon')]\n",
            "\n",
            "-- keys(), values() --\n",
            "keys(): ['a', 'b', 'a', 'c', 'b', 'a', 'd']\n",
            "values(): [1, 2, 3, 5, 1, 2, 4]\n",
            "\n",
            "-- mapValues, flatMapValues --\n",
            "mapValues(*10): [('a', 10), ('b', 20), ('a', 30), ('c', 50), ('b', 10), ('a', 20), ('d', 40)]\n",
            "flatMapValues(range(1, v+1)) sample: [('a', 1), ('b', 1), ('b', 2), ('a', 1), ('a', 2), ('a', 3), ('c', 1), ('c', 2), ('c', 3), ('c', 4), ('c', 5), ('b', 1)]\n",
            "\n",
            "-- reduceByKey, foldByKey --\n",
            "reduceByKey(sum): [('a', 6), ('b', 3), ('c', 5), ('d', 4)]\n",
            "foldByKey(sum, zero=0): [('a', 6), ('b', 3), ('c', 5), ('d', 4)]\n",
            "\n",
            "-- aggregateByKey (avg per key) --\n",
            "aggregateByKey -> (sum,count): [('a', (6, 3)), ('b', (3, 2)), ('c', (5, 1)), ('d', (4, 1))]\n",
            "avg_by_key: [('a', 2.0), ('b', 1.5), ('c', 5.0), ('d', 4.0)]\n",
            "\n",
            "-- combineByKey (avg per key) --\n",
            "combineByKey -> (sum,count): [('a', (6, 3)), ('b', (3, 2)), ('c', (5, 1)), ('d', (4, 1))]\n",
            "avg_by_key2: [('a', 2.0), ('b', 1.5), ('c', 5.0), ('d', 4.0)]\n",
            "\n",
            "-- groupByKey --\n",
            "groupByKey -> [('a', [1, 2, 3]), ('b', [1, 2]), ('c', [5]), ('d', [4])]\n",
            "\n",
            "-- sortByKey --\n",
            "sortByKey(asc): [('a', 1), ('a', 3), ('a', 2), ('b', 2), ('b', 1), ('c', 5), ('d', 4)]\n",
            "sortByKey(desc): [('d', 4), ('c', 5), ('b', 2), ('b', 1), ('a', 1), ('a', 3), ('a', 2)]\n",
            "\n",
            "-- subtractByKey --\n",
            "kv.subtractByKey(kv2): [('c', 5)]\n",
            "\n",
            "-- joins --\n",
            "join: [('a', (1, 'alpha')), ('a', (2, 'alpha')), ('a', (3, 'alpha')), ('b', (1, 'beta')), ('b', (2, 'beta')), ('d', (4, 'delta'))]\n",
            "leftOuterJoin: [('a', (1, 'alpha')), ('a', (3, 'alpha')), ('a', (2, 'alpha')), ('b', (2, 'beta')), ('b', (1, 'beta')), ('c', (5, None)), ('d', (4, 'delta'))]\n",
            "rightOuterJoin: [('a', (1, 'alpha')), ('a', (3, 'alpha')), ('a', (2, 'alpha')), ('b', (2, 'beta')), ('b', (1, 'beta')), ('d', (4, 'delta')), ('e', (None, 'epsilon'))]\n",
            "fullOuterJoin: [('a', (1, 'alpha')), ('a', (3, 'alpha')), ('a', (2, 'alpha')), ('b', (2, 'beta')), ('b', (1, 'beta')), ('c', (5, None)), ('d', (4, 'delta')), ('e', (None, 'epsilon'))]\n",
            "\n",
            "-- cogroup --\n",
            "cogroup: [('a', ([1, 2, 3], ['alpha'])), ('b', ([1, 2], ['beta'])), ('c', ([5], [])), ('d', ([4], ['delta'])), ('e', ([], ['epsilon']))]\n",
            "\n",
            "-- actions: countByKey, lookup --\n",
            "countByKey: {'a': 3, 'b': 2, 'c': 1, 'd': 1}\n",
            "lookup('a'): [1, 3, 2]\n",
            "\n",
            "-- sampleByKey --\n",
            "sampleByKey: [('a', 1), ('a', 2), ('a', 3), ('b', 2), ('c', 5)]\n",
            "\n",
            "-- partitionBy, repartitionAndSortWithinPartitions --\n",
            "partitionBy(2) -> partitions: 2\n",
            "repartitionAndSortWithinPartitions(2): [(0, [('b', 2), ('b', 1), ('c', 5), ('d', 4)]), (1, [('a', 1), ('a', 3), ('a', 2)])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bEWapYddseBf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}