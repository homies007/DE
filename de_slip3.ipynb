{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWVYL_pHlEBo",
        "outputId": "12c3f191-164d-4037-82af-5c65d763c877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- DataFrame with Date and Timestamp ---\n",
            "+---+----------+--------------------------+\n",
            "|id |today_date|current_ts                |\n",
            "+---+----------+--------------------------+\n",
            "|0  |2025-10-03|2025-10-03 05:42:21.410844|\n",
            "+---+----------+--------------------------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = false)\n",
            " |-- today_date: date (nullable = false)\n",
            " |-- current_ts: timestamp (nullable = false)\n",
            "\n",
            "\n",
            "--- DataFrame with Extracted Time Components ---\n",
            "+---+----------+--------------------------+----+------+------+\n",
            "|id |today_date|current_ts                |hour|minute|second|\n",
            "+---+----------+--------------------------+----+------+------+\n",
            "|0  |2025-10-03|2025-10-03 05:42:25.508282|5   |42    |25    |\n",
            "+---+----------+--------------------------+----+------+------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = false)\n",
            " |-- today_date: date (nullable = false)\n",
            " |-- current_ts: timestamp (nullable = false)\n",
            " |-- hour: integer (nullable = false)\n",
            " |-- minute: integer (nullable = false)\n",
            " |-- second: integer (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#1] a) Create a data frame with todayâ€™s date and timestamp\n",
        "#   b) Display the hours, minutes and seconds from the timestamp\n",
        "\n",
        "#Solution:\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import current_date, current_timestamp, hour, minute, second, col\n",
        "\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"DateTimeOperations\").getOrCreate()\n",
        "\n",
        "# a) Create a DataFrame with today's date and timestamp\n",
        "df = spark.range(1).withColumn(\"today_date\", current_date()) \\\n",
        "                   .withColumn(\"current_ts\", current_timestamp())\n",
        "\n",
        "print(\"--- DataFrame with Date and Timestamp ---\")\n",
        "df.show(truncate=False)\n",
        "df.printSchema()\n",
        "\n",
        "\n",
        "# b) Display the hours, minutes, and seconds from the timestamp\n",
        "time_df = df.withColumn(\"hour\", hour(col(\"current_ts\"))) \\\n",
        "            .withColumn(\"minute\", minute(col(\"current_ts\"))) \\\n",
        "            .withColumn(\"second\", second(col(\"current_ts\")))\n",
        "\n",
        "print(\"\\n--- DataFrame with Extracted Time Components ---\")\n",
        "time_df.show(truncate=False)\n",
        "time_df.printSchema()\n",
        "\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 For the following employee data showing name, dept and salary, perform the given operations:\n",
        "\n",
        "#a) Create a data frame for the above data\n",
        "#b) Display average salary\n",
        "#c) Display number of unique departments\n",
        "#d) Display number of employees with unique salary\n",
        "\n",
        "#Solution:\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. SETUP: Install PySpark and import all necessary modules\n",
        "# ==============================================================================\n",
        "!pip install pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession, Row\n",
        "from pyspark.sql.functions import col, flatten, explode, concat_ws, current_date, current_timestamp, hour, minute, second, avg, countDistinct\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. INITIALIZE SPARK SESSION\n",
        "# ==============================================================================\n",
        "spark = SparkSession.builder.appName(\"AllInOnePySpark\").getOrCreate()\n",
        "\n",
        "print(\"spark.sparkContext.appName:\", spark.sparkContext.appName)\n",
        "print(\"-\" * 60)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# PROBLEM 1: DataFrame Joins (Employee and Department)\n",
        "# ==============================================================================\n",
        "print(\"\\nPROBLEM 1: DataFrame Joins\")\n",
        "# Data Creation\n",
        "emp_data = [(1, \"Alice\", 10), (2, \"Bob\", 20), (3, \"Charlie\", 10), (4, \"David\", None), (5, \"Eve\", 40)]\n",
        "emp_columns = [\"emp_id\", \"emp_name\", \"dept_id\"]\n",
        "empDF = spark.createDataFrame(data=emp_data, schema=emp_columns)\n",
        "\n",
        "dept_data = [(10, \"Engineering\"), (20, \"Marketing\"), (30, \"Finance\")]\n",
        "dept_columns = [\"dept_id\", \"dept_name\"]\n",
        "deptDF = spark.createDataFrame(data=dept_data, schema=dept_columns)\n",
        "\n",
        "# a) Left Outer Join\n",
        "print(\"\\na) Left Outer Join:\")\n",
        "empDF.join(deptDF, on=\"dept_id\", how=\"left\").show()\n",
        "\n",
        "# b) Full Outer Join\n",
        "print(\"\\nb) Full Outer Join:\")\n",
        "empDF.join(deptDF, on=\"dept_id\", how=\"full\").show()\n",
        "\n",
        "# c) Inner Join\n",
        "print(\"\\nc) Inner Join:\")\n",
        "empDF.join(deptDF, on=\"dept_id\", how=\"inner\").show()\n",
        "print(\"-\" * 60)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# PROBLEM 2: Operations on Nested Schema DataFrame\n",
        "# ==============================================================================\n",
        "print(\"\\nPROBLEM 2: Operations on Nested Schema DataFrame\")\n",
        "# Schema and Data\n",
        "schema_nested = StructType([\n",
        "    StructField('name', StructType([\n",
        "         StructField('firstname', StringType(), True),\n",
        "         StructField('middlename', StringType(), True),\n",
        "         StructField('lastname', StringType(), True)\n",
        "         ])),\n",
        "     StructField('dob', StringType(), True),\n",
        "     StructField('gender', StringType(), True),\n",
        "     StructField('expenses', StringType(), True)\n",
        "])\n",
        "data_nested = [\n",
        "    Row(name=Row(\"James;\", \"\", \"Smith\"), dob=\"36636\", gender=\"M\", expenses=\"20000\"),\n",
        "    Row(name=Row(\"Michael\", \"Rose\", \"\"), dob=\"40288\", gender=\"M\", expenses=\"40000\"),\n",
        "    Row(name=Row(\"Jen\", \"Mary\", \"Brown\"), dob=\"\", gender=\"F\", expenses=\"-1\")\n",
        "]\n",
        "df_nested = spark.createDataFrame(data_nested, schema_nested)\n",
        "\n",
        "# Transformations\n",
        "final_df_nested = df_nested.withColumn(\"expenses\", col(\"expenses\").cast(IntegerType())) \\\n",
        "                           .withColumnRenamed(\"dob\", \"DateOfBirth\") \\\n",
        "                           .withColumn(\"expense_multiplied\", col(\"expenses\") * 5)\n",
        "\n",
        "print(\"\\nFinal DataFrame after transformations:\")\n",
        "final_df_nested.show(truncate=False)\n",
        "final_df_nested.printSchema()\n",
        "print(\"-\" * 60)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# PROBLEM 3: Operations on Nested Array DataFrame\n",
        "# ==============================================================================\n",
        "print(\"\\nPROBLEM 3: Operations on Nested Array DataFrame\")\n",
        "# Data and Schema\n",
        "data_array = [(\"Alice\", [[\"Math\", \"90\"], [\"Science\", \"85\"]]), (\"Bob\", [[\"History\", \"88\"], [\"English\", \"92\"]])]\n",
        "schema_array = StructType([StructField(\"name\", StringType()), StructField(\"subjects\", ArrayType(ArrayType(StringType())))])\n",
        "df_array = spark.createDataFrame(data_array, schema_array)\n",
        "print(\"\\nInitial Nested Array DataFrame:\")\n",
        "df_array.show(truncate=False)\n",
        "\n",
        "# a) Flatten Nested Array\n",
        "flattened_df = df_array.withColumn(\"flat_subjects\", flatten(col(\"subjects\")))\n",
        "print(\"\\na) Flattened Array:\")\n",
        "flattened_df.show(truncate=False)\n",
        "\n",
        "# b) Explode Nested Array\n",
        "print(\"\\nb) Exploded Array:\")\n",
        "df_array.withColumn(\"exploded_subjects\", explode(col(\"subjects\"))).show(truncate=False)\n",
        "\n",
        "# c) Convert Array to String\n",
        "print(\"\\nc) Array converted to String:\")\n",
        "flattened_df.withColumn(\"subjects_string\", concat_ws(\", \", col(\"flat_subjects\"))).show(truncate=False)\n",
        "print(\"-\" * 60)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# PROBLEM 4: Date and Timestamp Operations\n",
        "# ==============================================================================\n",
        "print(\"\\nPROBLEM 4: Date and Timestamp Operations\")\n",
        "# a) Create a DataFrame with todayâ€™s date and timestamp\n",
        "df_time = spark.range(1).withColumn(\"today_date\", current_date()) \\\n",
        "                       .withColumn(\"current_ts\", current_timestamp())\n",
        "print(\"\\na) DataFrame with Current Date and Timestamp:\")\n",
        "df_time.show(truncate=False)\n",
        "\n",
        "# b) Display the hours, minutes and seconds from the timestamp\n",
        "print(\"\\nb) Extracted Time Components:\")\n",
        "df_time.withColumn(\"hour\", hour(col(\"current_ts\"))) \\\n",
        "       .withColumn(\"minute\", minute(col(\"current_ts\"))) \\\n",
        "       .withColumn(\"second\", second(col(\"current_ts\"))).show()\n",
        "print(\"-\" * 60)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# PROBLEM 5: DataFrame Aggregations\n",
        "# ==============================================================================\n",
        "print(\"\\nPROBLEM 5: DataFrame Aggregations\")\n",
        "# a) Create DataFrame\n",
        "data_agg = [\n",
        "    (\"James\", \"Sales\", 3000), (\"Michael\", \"Sales\", 4600), (\"Robert\", \"Sales\", 4100),\n",
        "    (\"Maria\", \"Finance\", 3000), (\"James\", \"Sales\", 3000), (\"Scott\", \"Finance\", 3300),\n",
        "    (\"Jen\", \"Finance\", 3900), (\"Jeff\", \"Marketing\", 3000), (\"Kumar\", \"Marketing\", 2000),\n",
        "    (\"Saif\", \"Sales\", 4100), (\"Jason\", \"Sales\", 9000), (\"Alice\", \"Finance\", 3700),\n",
        "    (\"Jenniffer\", \"Finance\", 8900), (\"Jenson\", \"Marketing\", 9000)\n",
        "]\n",
        "schema_agg = [\"name\", \"department\", \"salary\"]\n",
        "df_agg = spark.createDataFrame(data_agg, schema_agg)\n",
        "print(\"\\nInitial DataFrame for Aggregation:\")\n",
        "df_agg.show(5)\n",
        "\n",
        "# b) Display average salary\n",
        "print(\"\\nb) Average Salary:\")\n",
        "df_agg.select(avg(\"salary\").alias(\"average_salary\")).show()\n",
        "\n",
        "# c) Display number of unique departments\n",
        "print(\"\\nc) Unique Department Count:\")\n",
        "df_agg.select(countDistinct(\"department\").alias(\"unique_department_count\")).show()\n",
        "\n",
        "# d) Display number of employees with unique salary\n",
        "print(\"\\nd) Unique Salary Count:\")\n",
        "df_agg.select(countDistinct(\"salary\").alias(\"unique_salary_count\")).show()\n",
        "print(\"-\" * 60)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. STOP THE SPARK SESSION\n",
        "# ==============================================================================\n",
        "spark.stop()\n",
        "print(\"\\nSpark session stopped.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuUIsiIGlj6y",
        "outputId": "f8c9aefb-104c-44d9-b4e0-4d4ad7d19fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "spark.sparkContext.appName: AllInOnePySpark\n",
            "------------------------------------------------------------\n",
            "\n",
            "PROBLEM 1: DataFrame Joins\n",
            "\n",
            "a) Left Outer Join:\n",
            "+-------+------+--------+-----------+\n",
            "|dept_id|emp_id|emp_name|  dept_name|\n",
            "+-------+------+--------+-----------+\n",
            "|     10|     1|   Alice|Engineering|\n",
            "|     20|     2|     Bob|  Marketing|\n",
            "|   NULL|     4|   David|       NULL|\n",
            "|     10|     3| Charlie|Engineering|\n",
            "|     40|     5|     Eve|       NULL|\n",
            "+-------+------+--------+-----------+\n",
            "\n",
            "\n",
            "b) Full Outer Join:\n",
            "+-------+------+--------+-----------+\n",
            "|dept_id|emp_id|emp_name|  dept_name|\n",
            "+-------+------+--------+-----------+\n",
            "|   NULL|     4|   David|       NULL|\n",
            "|     10|     1|   Alice|Engineering|\n",
            "|     10|     3| Charlie|Engineering|\n",
            "|     20|     2|     Bob|  Marketing|\n",
            "|     30|  NULL|    NULL|    Finance|\n",
            "|     40|     5|     Eve|       NULL|\n",
            "+-------+------+--------+-----------+\n",
            "\n",
            "\n",
            "c) Inner Join:\n",
            "+-------+------+--------+-----------+\n",
            "|dept_id|emp_id|emp_name|  dept_name|\n",
            "+-------+------+--------+-----------+\n",
            "|     10|     1|   Alice|Engineering|\n",
            "|     10|     3| Charlie|Engineering|\n",
            "|     20|     2|     Bob|  Marketing|\n",
            "+-------+------+--------+-----------+\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "PROBLEM 2: Operations on Nested Schema DataFrame\n",
            "\n",
            "Final DataFrame after transformations:\n",
            "+------------------+-----------+------+--------+------------------+\n",
            "|name              |DateOfBirth|gender|expenses|expense_multiplied|\n",
            "+------------------+-----------+------+--------+------------------+\n",
            "|{James;, , Smith} |36636      |M     |20000   |100000            |\n",
            "|{Michael, Rose, } |40288      |M     |40000   |200000            |\n",
            "|{Jen, Mary, Brown}|           |F     |-1      |-5                |\n",
            "+------------------+-----------+------+--------+------------------+\n",
            "\n",
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- DateOfBirth: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- expenses: integer (nullable = true)\n",
            " |-- expense_multiplied: integer (nullable = true)\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "PROBLEM 3: Operations on Nested Array DataFrame\n",
            "\n",
            "Initial Nested Array DataFrame:\n",
            "+-----+------------------------------+\n",
            "|name |subjects                      |\n",
            "+-----+------------------------------+\n",
            "|Alice|[[Math, 90], [Science, 85]]   |\n",
            "|Bob  |[[History, 88], [English, 92]]|\n",
            "+-----+------------------------------+\n",
            "\n",
            "\n",
            "a) Flattened Array:\n",
            "+-----+------------------------------+--------------------------+\n",
            "|name |subjects                      |flat_subjects             |\n",
            "+-----+------------------------------+--------------------------+\n",
            "|Alice|[[Math, 90], [Science, 85]]   |[Math, 90, Science, 85]   |\n",
            "|Bob  |[[History, 88], [English, 92]]|[History, 88, English, 92]|\n",
            "+-----+------------------------------+--------------------------+\n",
            "\n",
            "\n",
            "b) Exploded Array:\n",
            "+-----+------------------------------+-----------------+\n",
            "|name |subjects                      |exploded_subjects|\n",
            "+-----+------------------------------+-----------------+\n",
            "|Alice|[[Math, 90], [Science, 85]]   |[Math, 90]       |\n",
            "|Alice|[[Math, 90], [Science, 85]]   |[Science, 85]    |\n",
            "|Bob  |[[History, 88], [English, 92]]|[History, 88]    |\n",
            "|Bob  |[[History, 88], [English, 92]]|[English, 92]    |\n",
            "+-----+------------------------------+-----------------+\n",
            "\n",
            "\n",
            "c) Array converted to String:\n",
            "+-----+------------------------------+--------------------------+------------------------+\n",
            "|name |subjects                      |flat_subjects             |subjects_string         |\n",
            "+-----+------------------------------+--------------------------+------------------------+\n",
            "|Alice|[[Math, 90], [Science, 85]]   |[Math, 90, Science, 85]   |Math, 90, Science, 85   |\n",
            "|Bob  |[[History, 88], [English, 92]]|[History, 88, English, 92]|History, 88, English, 92|\n",
            "+-----+------------------------------+--------------------------+------------------------+\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "PROBLEM 4: Date and Timestamp Operations\n",
            "\n",
            "a) DataFrame with Current Date and Timestamp:\n",
            "+---+----------+--------------------------+\n",
            "|id |today_date|current_ts                |\n",
            "+---+----------+--------------------------+\n",
            "|0  |2025-10-03|2025-10-03 05:45:30.205404|\n",
            "+---+----------+--------------------------+\n",
            "\n",
            "\n",
            "b) Extracted Time Components:\n",
            "+---+----------+--------------------+----+------+------+\n",
            "| id|today_date|          current_ts|hour|minute|second|\n",
            "+---+----------+--------------------+----+------+------+\n",
            "|  0|2025-10-03|2025-10-03 05:45:...|   5|    45|    30|\n",
            "+---+----------+--------------------+----+------+------+\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "PROBLEM 5: DataFrame Aggregations\n",
            "\n",
            "Initial DataFrame for Aggregation:\n",
            "+-------+----------+------+\n",
            "|   name|department|salary|\n",
            "+-------+----------+------+\n",
            "|  James|     Sales|  3000|\n",
            "|Michael|     Sales|  4600|\n",
            "| Robert|     Sales|  4100|\n",
            "|  Maria|   Finance|  3000|\n",
            "|  James|     Sales|  3000|\n",
            "+-------+----------+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "b) Average Salary:\n",
            "+-----------------+\n",
            "|   average_salary|\n",
            "+-----------------+\n",
            "|4614.285714285715|\n",
            "+-----------------+\n",
            "\n",
            "\n",
            "c) Unique Department Count:\n",
            "+-----------------------+\n",
            "|unique_department_count|\n",
            "+-----------------------+\n",
            "|                      3|\n",
            "+-----------------------+\n",
            "\n",
            "\n",
            "d) Unique Salary Count:\n",
            "+-------------------+\n",
            "|unique_salary_count|\n",
            "+-------------------+\n",
            "|                  9|\n",
            "+-------------------+\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Spark session stopped.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VMZD2P-9mkda"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}